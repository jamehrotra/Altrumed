{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 1:  imports & config: pulls in pandas/numpy/sklearn, defines a simple RMSE helper and sets a random seed\"\"\"\n",
    "\n",
    "# Core\n",
    "import re, numpy as np, pandas as pd\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Utils\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SubjectID   r_value  skintone  Rad97-60/SpO2  SubjectNum\n",
      "0  Subject1  0.858301     1.992             91           1\n",
      "1  Subject1  0.831904     1.992             91           1\n",
      "2  Subject1  0.922036     1.992             90           1\n",
      "3  Subject1  0.925349     1.992             90           1\n",
      "4  Subject1  0.990698     1.992             89           1\n",
      "n rows: 604 | n subjects: 11\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cell 2:  load & inspect: reads cleaned CSV, standardizes column names, picks the target Sp02 column, drops NaNs/Infs, extracts an integer Subject ID for grouping, and prints basic info\"\"\"\n",
    "\n",
    "# Path to your mentor's cleaned file\n",
    "CSV = \"DOVE_Hypoxia_Data_Manually_Synced.csv\"  # change if needed\n",
    "\n",
    "df = pd.read_csv(CSV)\n",
    "\n",
    "# Standardize column names\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Choose which commercial SpO2 to use as ground truth (Rad97 or Nellcor)\n",
    "TARGET_COL = \"Rad97-60/SpO2\"             # or \"Nellcor PM1000N-1/SpO2\"\n",
    "\n",
    "# Keep only the columns we need and drop rows with NaN/inf\n",
    "use_cols = [\"SubjectID\", \"r_value\", \"skintone\", TARGET_COL]\n",
    "df = df[use_cols].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# Extract integer Subject number for grouping\n",
    "df[\"SubjectNum\"] = df[\"SubjectID\"].str.extract(r\"Subject(\\d+)\", expand=False).astype(int)\n",
    "\n",
    "print(df.head(5))\n",
    "print(\"n rows:\", len(df), \"| n subjects:\", df.SubjectNum.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 3:  LOSO helper: defines a generic “leave-one-subject-out” evaluation loop that trains on all-but-one subject and aggregates MAE/RMSE and correlation\"\"\"\n",
    "\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "\n",
    "def loso_eval(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    fit_predict_fn: Callable[[np.ndarray, np.ndarray, np.ndarray], np.ndarray],\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Generic LOSO evaluation loop.\n",
    "    fit_predict_fn must train on (X_train, y_train) and return predictions for X_test.\n",
    "    \"\"\"\n",
    "    logo = LeaveOneGroupOut()\n",
    "    maes, rmses, rs = [], [], []\n",
    "    all_true, all_pred = [], []\n",
    "\n",
    "    for tr, te in logo.split(X, y, groups):\n",
    "        y_tr, y_te = y[tr], y[te]\n",
    "        y_hat = fit_predict_fn(X[tr], y_tr, X[te])\n",
    "\n",
    "        maes.append(mean_absolute_error(y_te, y_hat))\n",
    "        rmses.append(rmse(y_te, y_hat))\n",
    "        # Pearson r on this fold (guard against constant arrays)\n",
    "        if np.std(y_hat) > 1e-9 and np.std(y_te) > 1e-9:\n",
    "            r = np.corrcoef(y_te, y_hat)[0,1]\n",
    "            rs.append(r)\n",
    "\n",
    "        all_true.append(y_te)\n",
    "        all_pred.append(y_hat)\n",
    "\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_pred = np.concatenate(all_pred)\n",
    "    overall_r = np.corrcoef(all_true, all_pred)[0,1] if (np.std(all_pred)>1e-9 and np.std(all_true)>1e-9) else np.nan\n",
    "\n",
    "    return {\n",
    "        \"MAE\": float(np.mean(maes)),\n",
    "        \"RMSE\": float(np.mean(rmses)),\n",
    "        \"Fold-r (mean)\": float(np.mean(rs)) if rs else np.nan,\n",
    "        \"Overall-r\": float(overall_r),\n",
    "        \"n_folds\": int(len(maes)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 2.616163582991273,\n",
       " 'RMSE': 3.2469400911066675,\n",
       " 'Fold-r (mean)': 0.966660491044379,\n",
       " 'Overall-r': 0.9174718236785232,\n",
       " 'n_folds': 11}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"Cell 4: baseline (quadratic): reproduces the Beer–Lambert style quadratic fit using only r_value, evaluated with LOSO so it’s a fair, per-subject baseline\"\"\"\n",
    "\n",
    "y = df[TARGET_COL].values\n",
    "groups = df[\"SubjectNum\"].values\n",
    "\n",
    "def quad_baseline_fit_predict(Xtr, ytr, Xte):\n",
    "    # X uses only r_value column for the baseline\n",
    "    r_tr = Xtr[:, 0]\n",
    "    r_te = Xte[:, 0]\n",
    "    # robust clean\n",
    "    m = ~np.isnan(r_tr) & ~np.isnan(ytr) & ~np.isinf(r_tr) & ~np.isinf(ytr)\n",
    "    coeffs = np.polyfit(r_tr[m], ytr[m], deg=2)\n",
    "    a, b, c = coeffs\n",
    "    return a * r_te**2 + b * r_te + c\n",
    "\n",
    "# Build feature matrix for baseline: only r_value\n",
    "X_base = df[[\"r_value\"]].values\n",
    "res_baseline = loso_eval(X_base, y, groups, quad_baseline_fit_predict)\n",
    "res_baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'MAE': 2.731902672314914,\n",
       "  'RMSE': 3.3757012123988632,\n",
       "  'Fold-r (mean)': 0.952601861995917,\n",
       "  'Overall-r': 0.9120807646422168,\n",
       "  'n_folds': 11},\n",
       " {'MAE': 3.6983185003838,\n",
       "  'RMSE': 4.40310209900018,\n",
       "  'Fold-r (mean)': 0.9609352808423934,\n",
       "  'Overall-r': 0.8648947465091292,\n",
       "  'n_folds': 11})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Cell 5 — ML #1 (Gradient Boosting): compares models with r_value alone vs r_value + skintone under LOSO to quantify the gain from adding skin tone\"\"\"\n",
    "\n",
    "def gbr_fit_predict(Xtr, ytr, Xte):\n",
    "    model = GradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "    model.fit(Xtr, ytr)\n",
    "    return model.predict(Xte)\n",
    "\n",
    "# (a) without skintone\n",
    "X_no_tone = df[[\"r_value\"]].values\n",
    "res_gbr_no = loso_eval(X_no_tone, y, groups, gbr_fit_predict)\n",
    "\n",
    "# (b) with skintone\n",
    "X_with_tone = df[[\"r_value\", \"skintone\"]].values\n",
    "res_gbr_with = loso_eval(X_with_tone, y, groups, gbr_fit_predict)\n",
    "\n",
    "res_gbr_no, res_gbr_with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'MAE': 2.8180136547536083,\n",
       "  'RMSE': 3.49785187910624,\n",
       "  'Fold-r (mean)': 0.9446105353733633,\n",
       "  'Overall-r': 0.9071317354078964,\n",
       "  'n_folds': 11},\n",
       " {'MAE': 3.2447717388620605,\n",
       "  'RMSE': 3.9250800672171233,\n",
       "  'Fold-r (mean)': 0.9560567549351021,\n",
       "  'Overall-r': 0.885240788698983,\n",
       "  'n_folds': 11})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Cell 6: ML #2 (Random Forest): repeats the same comparison with a tree ensemble to check robustness to nonlinearity and interactions\"\"\"\n",
    "\n",
    "def rf_fit_predict(Xtr, ytr, Xte):\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=400, max_depth=None, min_samples_leaf=2, random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "    model.fit(Xtr, ytr)\n",
    "    return model.predict(Xte)\n",
    "\n",
    "res_rf_no  = loso_eval(X_no_tone,   y, groups, rf_fit_predict)\n",
    "res_rf_with= loso_eval(X_with_tone, y, groups, rf_fit_predict)\n",
    "\n",
    "res_rf_no, res_rf_with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'MAE': 2.6188237248363344,\n",
       "  'RMSE': 3.2153055778345423,\n",
       "  'Fold-r (mean)': 0.9727555394271806,\n",
       "  'Overall-r': 0.9152727187442256,\n",
       "  'n_folds': 11},\n",
       " {'MAE': 2.763944350481047,\n",
       "  'RMSE': 3.327641910428741,\n",
       "  'Fold-r (mean)': 0.9728528176281518,\n",
       "  'Overall-r': 0.9117881485005682,\n",
       "  'n_folds': 11})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Cell 7: ML #3 (Ridge + polynomial r): fits a linear model on polynomial features of r_value (degree 3), with and without raw skintone, again under LOSO\"\"\"\n",
    "\n",
    "# Polynomial on r_value up to degree 3; optionally append raw skintone\n",
    "def ridge_poly_fit_predict(Xtr, ytr, Xte, include_tone: bool):\n",
    "    # Split columns: 0=r_value, 1=skintone (if present)\n",
    "    r_tr = Xtr[:, 0:1]\n",
    "    r_te = Xte[:, 0:1]\n",
    "    poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    R_tr = poly.fit_transform(r_tr)\n",
    "    R_te = poly.transform(r_te)\n",
    "\n",
    "    if include_tone and Xtr.shape[1] > 1:\n",
    "        R_tr = np.hstack([R_tr, Xtr[:, 1:2]])\n",
    "        R_te = np.hstack([R_te, Xte[:, 1:2]])\n",
    "\n",
    "    model = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "    model.fit(R_tr, ytr)\n",
    "    return model.predict(R_te)\n",
    "\n",
    "res_ridge_no   = loso_eval(X_no_tone,   y, groups, lambda a,b,c: ridge_poly_fit_predict(a,b,c, include_tone=False))\n",
    "res_ridge_with = loso_eval(X_with_tone, y, groups, lambda a,b,c: ridge_poly_fit_predict(a,b,c, include_tone=True))\n",
    "\n",
    "res_ridge_no, res_ridge_with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Fold-r (mean)</th>\n",
       "      <th>Overall-r</th>\n",
       "      <th>n_folds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline Quad (r only)</td>\n",
       "      <td>2.616164</td>\n",
       "      <td>3.246940</td>\n",
       "      <td>0.966660</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Poly (r only)</td>\n",
       "      <td>2.618824</td>\n",
       "      <td>3.215306</td>\n",
       "      <td>0.972756</td>\n",
       "      <td>0.915273</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBR (r only)</td>\n",
       "      <td>2.731903</td>\n",
       "      <td>3.375701</td>\n",
       "      <td>0.952602</td>\n",
       "      <td>0.912081</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge Poly (+skintone)</td>\n",
       "      <td>2.763944</td>\n",
       "      <td>3.327642</td>\n",
       "      <td>0.972853</td>\n",
       "      <td>0.911788</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RF (r only)</td>\n",
       "      <td>2.818014</td>\n",
       "      <td>3.497852</td>\n",
       "      <td>0.944611</td>\n",
       "      <td>0.907132</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RF (+skintone)</td>\n",
       "      <td>3.244772</td>\n",
       "      <td>3.925080</td>\n",
       "      <td>0.956057</td>\n",
       "      <td>0.885241</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GBR (+skintone)</td>\n",
       "      <td>3.698319</td>\n",
       "      <td>4.403102</td>\n",
       "      <td>0.960935</td>\n",
       "      <td>0.864895</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model       MAE      RMSE  Fold-r (mean)  Overall-r  \\\n",
       "0  Baseline Quad (r only)  2.616164  3.246940       0.966660   0.917472   \n",
       "1     Ridge Poly (r only)  2.618824  3.215306       0.972756   0.915273   \n",
       "2            GBR (r only)  2.731903  3.375701       0.952602   0.912081   \n",
       "3  Ridge Poly (+skintone)  2.763944  3.327642       0.972853   0.911788   \n",
       "4             RF (r only)  2.818014  3.497852       0.944611   0.907132   \n",
       "5          RF (+skintone)  3.244772  3.925080       0.956057   0.885241   \n",
       "6         GBR (+skintone)  3.698319  4.403102       0.960935   0.864895   \n",
       "\n",
       "   n_folds  \n",
       "0       11  \n",
       "1       11  \n",
       "2       11  \n",
       "3       11  \n",
       "4       11  \n",
       "5       11  \n",
       "6       11  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Cell 8:  summary table: collates all LOSO metrics into one sortable DataFrame so you can see which approach (and whether adding skin tone) wins on MAE/RMSE/\"\"\"\n",
    "\n",
    "rows = []\n",
    "rows.append((\"Baseline Quad (r only)\", res_baseline))\n",
    "rows.append((\"GBR (r only)\",          res_gbr_no))\n",
    "rows.append((\"GBR (+skintone)\",       res_gbr_with))\n",
    "rows.append((\"RF (r only)\",           res_rf_no))\n",
    "rows.append((\"RF (+skintone)\",        res_rf_with))\n",
    "rows.append((\"Ridge Poly (r only)\",   res_ridge_no))\n",
    "rows.append((\"Ridge Poly (+skintone)\",res_ridge_with))\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\"Model\": name, **metrics} for name, metrics in rows\n",
    "]).sort_values(\"MAE\")\n",
    "summary.reset_index(drop=True, inplace=True)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample MAE: 1.1006769002671426 RMSE: 1.4809196236707376\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cell 9: fit best model: trains your chosen best model on all data (often r_value + skintone) and prints in-sample error for quick sanity-check/plotting.\"\"\"\n",
    "\n",
    "# Choose the best performing setting from the table above\n",
    "best_features = [\"r_value\", \"skintone\"]  # update if your best model didn't use tone\n",
    "best_X = df[best_features].values\n",
    "\n",
    "best_model = GradientBoostingRegressor(random_state=RANDOM_STATE)  # swap if another model won\n",
    "best_model.fit(best_X, y)\n",
    "y_pred_full = best_model.predict(best_X)\n",
    "\n",
    "print(\"In-sample MAE:\", mean_absolute_error(y, y_pred_full), \"RMSE:\", rmse(y, y_pred_full))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altrumed_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
